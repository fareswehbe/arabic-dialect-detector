{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26e92bd8-415e-4974-8c98-25fc49194f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd1ad26a-d2ac-433c-be26-d72f742eed46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>dialect</th>\n",
       "      <th>sentence_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>غير احنا هاكا نبغو نزيدو عليها شفتي في تركيا ا...</td>\n",
       "      <td>DZ</td>\n",
       "      <td>غير احنا هاكا نبغو نزيدو عليها شفتي تركيا البر...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>حبيبتي بقلك انسي سنان لان فانزو من يوم ماعملو ...</td>\n",
       "      <td>DZ</td>\n",
       "      <td>حبيبتي بقلك انسي سنان لان فانزو يوم ماعملو فيه...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لازم ما تسكتيش على غلطي</td>\n",
       "      <td>PL</td>\n",
       "      <td>لازم تسكتيش غلطي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>أبشري ماطلبتي الا عزك منتخبنا هو منتخبكم الوعد...</td>\n",
       "      <td>QA</td>\n",
       "      <td>أبشري ماطلبتي الا عزك منتخبنا منتخبكم الوعد ال...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>حتى بالكشخة يعلمهم بو عدنان اله يرحمه</td>\n",
       "      <td>BH</td>\n",
       "      <td>حتى بالكشخة يعلمهم بو عدنان يرحمه</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence dialect  \\\n",
       "0  غير احنا هاكا نبغو نزيدو عليها شفتي في تركيا ا...      DZ   \n",
       "1  حبيبتي بقلك انسي سنان لان فانزو من يوم ماعملو ...      DZ   \n",
       "2                            لازم ما تسكتيش على غلطي      PL   \n",
       "3  أبشري ماطلبتي الا عزك منتخبنا هو منتخبكم الوعد...      QA   \n",
       "4              حتى بالكشخة يعلمهم بو عدنان اله يرحمه      BH   \n",
       "\n",
       "                                      sentence_clean  \n",
       "0  غير احنا هاكا نبغو نزيدو عليها شفتي تركيا البر...  \n",
       "1  حبيبتي بقلك انسي سنان لان فانزو يوم ماعملو فيه...  \n",
       "2                                   لازم تسكتيش غلطي  \n",
       "3  أبشري ماطلبتي الا عزك منتخبنا منتخبكم الوعد ال...  \n",
       "4                  حتى بالكشخة يعلمهم بو عدنان يرحمه  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df=pd.read_csv('../data/new_arabic_dialect_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef38a392-2864-40b0-ac1d-71f2cc5cd575",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>dialect</th>\n",
       "      <th>sentence_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>غير احنا هاكا نبغو نزيدو عليها شفتي في تركيا ا...</td>\n",
       "      <td>DZ</td>\n",
       "      <td>غير احنا هاكا نبغو نزيدو عليها شفتي تركيا البر...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>حبيبتي بقلك انسي سنان لان فانزو من يوم ماعملو ...</td>\n",
       "      <td>DZ</td>\n",
       "      <td>حبيبتي بقلك انسي سنان لان فانزو يوم ماعملو فيه...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لازم ما تسكتيش على غلطي</td>\n",
       "      <td>PL</td>\n",
       "      <td>لازم تسكتيش غلطي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>أبشري ماطلبتي الا عزك منتخبنا هو منتخبكم الوعد...</td>\n",
       "      <td>QA</td>\n",
       "      <td>أبشري ماطلبتي الا عزك منتخبنا منتخبكم الوعد ال...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>حتى بالكشخة يعلمهم بو عدنان اله يرحمه</td>\n",
       "      <td>BH</td>\n",
       "      <td>حتى بالكشخة يعلمهم بو عدنان يرحمه</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence dialect  \\\n",
       "0  غير احنا هاكا نبغو نزيدو عليها شفتي في تركيا ا...      DZ   \n",
       "1  حبيبتي بقلك انسي سنان لان فانزو من يوم ماعملو ...      DZ   \n",
       "2                            لازم ما تسكتيش على غلطي      PL   \n",
       "3  أبشري ماطلبتي الا عزك منتخبنا هو منتخبكم الوعد...      QA   \n",
       "4              حتى بالكشخة يعلمهم بو عدنان اله يرحمه      BH   \n",
       "\n",
       "                                      sentence_clean  \n",
       "0  غير احنا هاكا نبغو نزيدو عليها شفتي تركيا البر...  \n",
       "1  حبيبتي بقلك انسي سنان لان فانزو يوم ماعملو فيه...  \n",
       "2                                   لازم تسكتيش غلطي  \n",
       "3  أبشري ماطلبتي الا عزك منتخبنا منتخبكم الوعد ال...  \n",
       "4                  حتى بالكشخة يعلمهم بو عدنان يرحمه  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows containing NaN values\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0e97d7c-ea52-48bc-b45b-678b8df4f57a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "df['dialect'] = label_encoder.fit_transform(df['dialect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7859057-6646-472c-9992-65c9df95d5d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['sentence_clean'], df['dialect'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38cacd5b-f7ce-4d8e-bff5-b9d197387fed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pad the sequences to have the same length\n",
    "max_len = 100\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02e68db8-7d6e-40fc-b48f-d3871a3d908d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 128, input_length=max_len))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(20, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feadba40-5d8e-4e66-ad05-5bae16d83617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c11afab1-7244-44e7-91e1-84224c4f618c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5852/5852 [==============================] - 352s 60ms/step - loss: 1.1104 - accuracy: 0.6425 - val_loss: 1.8846 - val_accuracy: 0.4741\n",
      "Epoch 2/10\n",
      "5852/5852 [==============================] - 348s 59ms/step - loss: 1.0820 - accuracy: 0.6522 - val_loss: 1.9316 - val_accuracy: 0.4699\n",
      "Epoch 3/10\n",
      "5852/5852 [==============================] - 352s 60ms/step - loss: 1.0542 - accuracy: 0.6608 - val_loss: 1.9666 - val_accuracy: 0.4700\n",
      "Epoch 4/10\n",
      "5852/5852 [==============================] - 361s 62ms/step - loss: 1.0300 - accuracy: 0.6681 - val_loss: 2.0050 - val_accuracy: 0.4681\n",
      "Epoch 5/10\n",
      "5852/5852 [==============================] - 354s 61ms/step - loss: 1.0101 - accuracy: 0.6736 - val_loss: 2.0415 - val_accuracy: 0.4659\n",
      "Epoch 6/10\n",
      "5852/5852 [==============================] - 356s 61ms/step - loss: 0.9899 - accuracy: 0.6800 - val_loss: 2.0773 - val_accuracy: 0.4650\n",
      "Epoch 7/10\n",
      "5852/5852 [==============================] - 362s 62ms/step - loss: 0.9726 - accuracy: 0.6858 - val_loss: 2.1007 - val_accuracy: 0.4636\n",
      "Epoch 8/10\n",
      "5852/5852 [==============================] - 369s 63ms/step - loss: 0.9575 - accuracy: 0.6901 - val_loss: 2.1291 - val_accuracy: 0.4613\n",
      "Epoch 9/10\n",
      "5852/5852 [==============================] - 361s 62ms/step - loss: 0.9425 - accuracy: 0.6948 - val_loss: 2.1626 - val_accuracy: 0.4623\n",
      "Epoch 10/10\n",
      "5852/5852 [==============================] - 356s 61ms/step - loss: 0.9299 - accuracy: 0.6985 - val_loss: 2.1821 - val_accuracy: 0.4597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../models/model_deep.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=50, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, '../models/model_deep.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4c14f73-e4c1-4ada-a362-916648940461",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1171/1171 [==============================] - 10s 8ms/step - loss: 6.1814 - accuracy: 0.0892\n",
      "Test loss: 6.181434631347656\n",
      "Test accuracy: 0.08924359828233719\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "saved_model_deep = joblib.load('../models/model_deep.pkl')\n",
    "\n",
    "loss, accuracy = saved_model_deep.evaluate(X_test, y_test, batch_size=50)\n",
    "print(\"Test loss:\", loss)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf4f39d-1cf9-4cf6-a1a9-d9227497b7ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_data = [\"أنا بحب اللحمة\", \"بدي اروح عالسينما\"]\n",
    "new_data_sequences = tokenizer.texts_to_sequences(new_data)\n",
    "new_data_padded = pad_sequences(new_data_sequences, maxlen=max_len)\n",
    "predictions = saved_model_deep.predict(new_data_padded)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcd8cbc-4941-4714-9e1a-38b228049beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
